[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "No matching items\n\n\n    \n        \n        Subscribe\n        * indicates required\n\n    Email Address  *"
  },
  {
    "objectID": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html",
    "href": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html",
    "title": "Demystifying Document Question-Answering Chatbot - A Comprehensive Step-by-Step Tutorial with LangChain",
    "section": "",
    "text": "Recently I presented at REA Unstack‚Äôd on Large Language Models. It was mostly a demo about a ChatBot that I‚Äôve been experimenting with at work. This ChatBot can answer Australian property related questions and was built using publicly available data from our company - PropTrack.\nLater on, we also had a panel discussion on use of LLMs for corporates. We discussed about latest research, safety, deployment & all things LLM.\n\n\n\nREA Unstack‚Äôd\n\n\nMeet Sachin Abeywardana & Ned Letcher, our panelists.\nThere are many tutorials available today that showcase how to build a Q/A ChatBot, and most (if not all) use LangChain. Over the past few months, this framework has become extremely popular among all who want to use LLMs. But, its source code is hard to read and if you are trying to do something that‚Äôs not within the capabilities of the framework, it becomes extremely difficult.\n\n\nHere's a few thoughts on (LangChainAI?), the problems I see with it currently, and how I think it could improve. This was originally formatted as a message to (hwchase17?):Here's a few things off the top of my head ‚Äì 1. Heavy use of OOP. Having multiple layers of abstraction‚Ä¶\n\n‚Äî Sam Hogan ((0xSamHogan?)) July 12, 2023\n\n\nI recently wrote about LLMChains in langchain too, and found the same to true. You can find the previous blog post here. I would highly recommend the readers to give the previous blog post a read, it will explain LLMChains and Chains in langchain, that will be instrumental in understanding conversational chatbot that we are building today.\n\n\n\n\n\n\nNote\n\n\n\nüëâ This whole blog post is written with commit-id 24c165420827305e813f4b6d501f93d18f6d46a4. The blog post in itself is a completely working jupyter notebook with code-snippets."
  },
  {
    "objectID": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html#introduction",
    "href": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html#introduction",
    "title": "Demystifying Document Question-Answering Chatbot - A Comprehensive Step-by-Step Tutorial with LangChain",
    "section": "",
    "text": "Recently I presented at REA Unstack‚Äôd on Large Language Models. It was mostly a demo about a ChatBot that I‚Äôve been experimenting with at work. This ChatBot can answer Australian property related questions and was built using publicly available data from our company - PropTrack.\nLater on, we also had a panel discussion on use of LLMs for corporates. We discussed about latest research, safety, deployment & all things LLM.\n\n\n\nREA Unstack‚Äôd\n\n\nMeet Sachin Abeywardana & Ned Letcher, our panelists.\nThere are many tutorials available today that showcase how to build a Q/A ChatBot, and most (if not all) use LangChain. Over the past few months, this framework has become extremely popular among all who want to use LLMs. But, its source code is hard to read and if you are trying to do something that‚Äôs not within the capabilities of the framework, it becomes extremely difficult.\n\n\nHere's a few thoughts on (LangChainAI?), the problems I see with it currently, and how I think it could improve. This was originally formatted as a message to (hwchase17?):Here's a few things off the top of my head ‚Äì 1. Heavy use of OOP. Having multiple layers of abstraction‚Ä¶\n\n‚Äî Sam Hogan ((0xSamHogan?)) July 12, 2023\n\n\nI recently wrote about LLMChains in langchain too, and found the same to true. You can find the previous blog post here. I would highly recommend the readers to give the previous blog post a read, it will explain LLMChains and Chains in langchain, that will be instrumental in understanding conversational chatbot that we are building today.\n\n\n\n\n\n\nNote\n\n\n\nüëâ This whole blog post is written with commit-id 24c165420827305e813f4b6d501f93d18f6d46a4. The blog post in itself is a completely working jupyter notebook with code-snippets."
  },
  {
    "objectID": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html#chatbot-implementation-in-langchain",
    "href": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html#chatbot-implementation-in-langchain",
    "title": "Demystifying Document Question-Answering Chatbot - A Comprehensive Step-by-Step Tutorial with LangChain",
    "section": "2 Chatbot: Implementation in langchain",
    "text": "2 Chatbot: Implementation in langchain\nLet‚Äôs say you have a number of documents, in my case, I have a bunch of markdown documents. And we want to build a question answering chatbot that can take in a question, and find the answer based on the documents.\n\n\n\nFigure¬†1: Chatbot architecture\n\n\nIn essence, the chatbot looks something like above. We pass the documents through an ‚Äúembedding model‚Äù. It is easy enough to use OpenAI‚Äôs embedding API to convert documents, or chunks of documents to embeddings. These embeddings can be stored in a vector database such as Chroma, Faiss or Lance.\nThe user interacts through a ‚Äúchat interface‚Äù and enters a question/query. This query can also be converted to an embedding using the embedding model. Next, we can find the nearest chunks (similar to the query) using similarity search, then pass these nearest chunks (referred to as ‚Äúcontext‚Äù) to a Large Language Model such as ChatGPT.\nFinally, we retrieve an answer and this answer get‚Äôs passed back to the user in the chat interfact. We store this interaction in chat history and continue.\nThat is all in theory, in code, using langchain, above would look like:\n\n%load_ext autoreload\n%autoreload 2\n\nfrom langchain.vectorstores.chroma import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\n\n# directory to store vector database\npersist_directory = \".db/\"\nopenai_api_key = os.environ['OPENAI_API_KEY']\n# loader that loads `markdown` documents\nloader = DirectoryLoader(\"./output/\", glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader)\n# text splitter converts documents to chunks\ndocs = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\nchunks = text_splitter.split_documents(docs)\n# embedding model to convert chunks to embeddings\nembeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n\n# load vector database, uncomment below two lines if you'd like to create it\n\n#################### run only once at beginning ####################\n# db = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=persist_directory)\n# db.persist()\n####################################################################\ndb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\nmemory = ConversationBufferMemory(\n    memory_key=\"chat_history\", output_key='answer', return_messages=False)\n\n# create QA chain using `langchain`, database is used as vector store retriever to find \"context\" (using similarity search)\nqa = ConversationalRetrievalChain.from_llm(\n    llm=ChatOpenAI(temperature=0.2, model_name='gpt-3.5-turbo'),\n    chain_type=\"stuff\",\n    retriever=db.as_retriever(),\n    get_chat_history=lambda o:o,\n    memory=memory,\n    return_generated_question=True,\n    verbose=False,\n)\n\n\n# let's ask a question\nqa({\"question\": \"Why is it so hard to find a rental property in Australia in June 2023?\", \"chat_history\": []})\n\n{'question': 'Why is it so hard to find a rental property in Australia in June 2023?',\n 'chat_history': '',\n 'answer': 'In June 2023, it is hard to find a rental property in Australia due to several factors. Firstly, vacancy rates have fallen to very low levels across the country since the pandemic, meaning there is a shortage of available rentals. This is particularly evident in cities like Sydney and Melbourne. \\n\\nAdditionally, the departure of investors from the rental market has impacted rental supply. Many investors chose to sell their rental properties during 2020 and 2021, and there are few new investors entering the market to replace them. \\n\\nOn the other hand, demand for rentals has been strong in many parts of the country, especially in inner-city areas. The return of international students, migrants, and office workers to CBDs has led to a surge in demand for rental properties. \\n\\nOverall, these factors have created a tight rental market with low vacancy rates and increasing rental prices, making it difficult for individuals to find a rental property in Australia in June 2023.',\n 'generated_question': 'Why is it so hard to find a rental property in Australia in June 2023?'}\n\n\nLooking at the answer above, it really answers the question - ‚ÄúWhy is it so hard to find a rental property in Australia in June 2023?‚Äù very well. Above might only be a few lines of code, but there is actually quite a lot going on underneath. Refer to Figure¬†1 for everything that‚Äôs going on underneath.\nBut, as a recap, and matching our steps with code shared above:\n\nLoad markdown files in a list loader = DirectoryLoader(\"./output/\", glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader)\nCreate a splitter that can split documents to chunks text_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\nConvert each chunk and store as Embeddings in a Chroma DB Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=persist_directory)\nUse the database as retriever to get relevant text (context), and based on ‚Äòquestion‚Äô, use OpenAI‚Äôs gpt-3.5-turbo (ChatGPT) model to answer question based on context.\n\nConversationalRetrievalChain.from_llm(\n    llm=ChatOpenAI(temperature=0.2, model_name='gpt-3.5-turbo'),\n    chain_type=\"stuff\",\n    retriever=db.as_retriever(),\n    memory=memory,\n    verbose=False,\n)\n\nAlso store conversation as chat history in memory memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=False)\n\n\n2.1 Text splitter\nFor our simple usecase, we are using a text splitter of type CharacterTextSplitter.\ntext_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\nWe are using a chunk_size of 1024, which means that the document will be divided into chunks of size 1024, and there will be 128 character overlap between each of the chunks.\nThe CharacterTextSplitter used above splits texts based using regex and a separator. The separator in this case is '\\n\\n'. Thus, anytime there are two line breaks, our text splitter will split documents. Internally, in LangChain to split a text, _split_text_with_regex is being called.\n# simplified version without `keep_separator`\ndef _split_text_with_regex(\n    text: str, separator: str, keep_separator: bool\n) -&gt; List[str]:\n    # Now that we have the separator, split the text\n    if separator:\n                splits = re.split(separator, text)\n    else:\n        splits = list(text)\n    return [s for s in splits if s != \"\"]\nThere are many other text splitters that we could have also used. For a complete list - refer here.\n\n\n\n\n\n\nNote\n\n\n\nOne good one to further try would be - MarkdownHeaderTextSplitter. This particular splitter splits based on markdown headings, and it might be more useful for our usecase.\n\nRemember, the idea of chunking is to keep text with common context together.\n\n\n\nNow, that we have created our first bit, a text splitter that can split documents to chunks, let‚Äôs move on to the embedding model.\n\n\n2.2 Embedding model\nAlso, for our embedding model - we are using OpenAIEmbeddings. The main idea for the embedding model is to convert the chunks from before to embeddings.\nRemember, an embedding is only a vector representation of the text.\nSo, how do we convert our chunks (few sentences long) to a bunch of numbers? We can use openai‚Äôs embeddings API. Without langchain, this looks something like:\n\nimport openai\nchunk = \"This is a sample chunk consisting of few sentences.\"\ndef get_embedding(text, model=\"text-embedding-ada-002\"):\n   text = text.replace(\"\\n\", \" \")\n   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n\nemb = get_embedding(chunk, model='text-embedding-ada-002')\nlen(emb)\n\n1536\n\n\nIn langchain, to achieve the same we instantiate from OpenAIEmbeddings.\n\nfrom langchain.embeddings import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings()\ntext = \"This is a test document.\"\nquery_result = embeddings.embed_query(text)\nlen(query_result)\n\n1536\n\n\nNow, to embed all chunks at once, OpenAIEmbeddings has a method called embed_documents.\n\nfrom langchain.embeddings import OpenAIEmbeddings\nimport numpy as np\nembeddings = OpenAIEmbeddings()\ndocs = [\"This is test document 1.\", \"This is test document 2.\"]\nembs = embeddings.embed_documents(docs)\nnp.array(embs).shape\n\n(2, 1536)\n\n\nGreat, now that we have a way to embed all documents, let‚Äôs look at vector database next.\n\n\n2.3 Vector database\nConsider the vector database to a repository of knowledge. All our chunks get converted to embeddings and get stored in a vector-db. In our case, we are using chroma-db.\nLooking at the documentation, we start by creating a client, and then a collection. Once we have a collection ready, it is very simple to query the collection to get back the results.\nresults = collection.query(\n    query_texts=[\"This is a query document\"],\n    n_results=2\n)\nWhat goes under the hood inside langchain, is that we first instantiate a chroma-db collection. Next, we use collection‚Äôs upsert method passing in embeddings and texts. And this way, we have created our vector database that can be used to find nearest chunks from our documents based on ‚Äúquery‚Äù using similarity-search.\n\n\n\n\n\n\nNote\n\n\n\n‚ùì Some questions here to ask would be\n\nWould results look different or better if we used Cohere Embeddings? What would be the price difference?\nWhat would the quality of results be like if we used open source models like Llama-v2 released a few days ago?\nWhat if we used sentence-transformers?\nDo we really need a vector database? Can we store the embeddings as a np.array and use cosine-similarity to find nearest embeddings?\n\n\n\n\n\n2.4 Q&A ChatBot\nSo far we have looked at text-splitter, embedding model and vector database. These are the building blocks of the chatbot. But, how do we bring the building blocks together?\nIn langchain, all the pieces come together in ConversationalRetrievalChain which is the main topic of this blog post too. We instantiate an instance of the class using @classmethod called from_llm.\nqa = ConversationalRetrievalChain.from_llm(\n    llm=OpenAIChat(temperature=0, max_tokens=-1),\n    chain_type=\"stuff\",\n    retriever=db.as_retriever(),\n    memory=memory,\n    get_chat_history=lambda x: x,\n    verbose=True,\n)\nresponse = qa({\n    \"question\": \"Why is it so hard to find a rental property in Australia in June 2023?\", \n    \"chat_history\": []\n})\nThere are two main things that go on inside a conversational retrieval chain.\nA conversational retrieval chain can take in a query, and based on the input query (question) and chat-history, it updates it to a new question.\nThis new question is then passed to a second document chain, to find the nearest chunks (based on question) - referred to as ‚Äúcontext‚Äù, and this context alongside the new question get‚Äôs passed to a large language model (such as gpt-3.5-turbo or ChatGPT), to retrieve the answer.\nSo, internally - ConversationalRetrievalChain consists of two chains:\n\nA question generator chain, which updates input query/question based on chat history (LLMChain)\nAnd a document chain to join retrieved documents/chunks together (StuffDocumentsChain)\n\n\n\n\n\n\n\nOn LLMChains\n\n\n\nGood news! We have already covered LLMChains in our previous blog post before here. In essence, given a prompt, the LLMChain can be used to generate an answer based on the prompt.\nGoing forward, I am going to assume that the reader has read the previous blog post and has a solid understanding of LLMChains & Chains in general.\n\n\nFrom our previous blog post, we know that anytime we call any chain in langchain, the __call__ method from Chain class gets invoked which in turn makes a call to _call method of derived class.\nThe ConversationalRetrievalChain is a subclass of BaseConversationalRetrievalChain which in turn is a subclass of Chain.\nThe _call method is implemented inside BaseConversationalRetrievalChain and it looks like below:\n    def _call(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -&gt; Dict[str, Any]:\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n        question = inputs[\"question\"]\n        get_chat_history = self.get_chat_history or _get_chat_history\n        chat_history_str = get_chat_history(inputs[\"chat_history\"])\n\n        if chat_history_str:\n            callbacks = _run_manager.get_child()\n            new_question = self.question_generator.run(\n                question=question, chat_history=chat_history_str, callbacks=callbacks\n            )\n        else:\n            new_question = question\n        accepts_run_manager = (\n            \"run_manager\" in inspect.signature(self._get_docs).parameters\n        )\n        if accepts_run_manager:\n            docs = self._get_docs(new_question, inputs, run_manager=_run_manager)\n        else:\n            docs = self._get_docs(new_question, inputs)  # type: ignore[call-arg]\n        new_inputs = inputs.copy()\n        if self.rephrase_question:\n            new_inputs[\"question\"] = new_question\n        new_inputs[\"chat_history\"] = chat_history_str\n        answer = self.combine_docs_chain.run(\n            input_documents=docs, callbacks=_run_manager.get_child(), **new_inputs\n        )\n        output: Dict[str, Any] = {self.output_key: answer}\n        if self.return_source_documents:\n            output[\"source_documents\"] = docs\n        if self.return_generated_question:\n            output[\"generated_question\"] = new_question\n        return output\nIn simple terms, first, the question_generator chain is called that updates the input question/query based on chat history.\nNext, we retrieve the documents based on our new_question using similarity search.\nThese retrieved docs, then get passed to combine_docs_chain which combines the retrieved chunks and passes them over to a large language model (in this case gpt-3.5-turbo) to get back the answer.\nLet‚Äôs understand both chains one by one in the next two sections. That way, we will be able to have a solid understanding of our conversational retrieval chain.\n\n2.4.1 Question generator chain\nLet‚Äôs start out with the question generator. Remeber, the question generator takes in the user question and a chat history, and based on chat history, it updates the question to a new question.\nWhy does it do that? The question generator rephrases the original question to be a standalone question. So if it is a follow up question like ‚ÄúWhy did that happen?‚Äù from the user, remember, we do not know what ‚Äúthat‚Äù is in this particular question.\nSo, what the question generator will do, is that it will look at the chat history, and fill information for the word ‚Äúthat‚Äù to update the question to be a standalone question. So the new question could be ‚ÄúWhy did the rental prices increase in Australia?‚Äù based on chat history.\nWe will also be looking at a working example of this in our code in this section.\nFrom a code perspective, in langchain, the question_generator is an instance of LLMChain.\nIn this case the prompt for the question generator (LLMChain) is CONDENSE_QUESTION_PROMPT which looks like:\n_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n\nChat History:\n{chat_history}\n\nFollow Up Input: {question}\nStandalone question:\"\"\"\n\nCONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\nSo taking in a chat_history and the original question (from the user), internally a new question get‚Äôs generated! This new question is a standalone question as discussed at the start of this section.\nLet‚Äôs see it in action. Let‚Äôs see how the original question get‚Äôs updated to a new question based on chat_history. Remember, the first time we interact with the question answer bot, chat history is NULL, so no new question is generated. But, it works from the second time forward.\nWe can get langchain to return the newly generated question by passing in return_generated_question=True to the ConversationRetrievalChain.\n\nqa.memory.chat_memory.messages\n\n[HumanMessage(content='Why is it so hard to find a rental property in Australia in June 2023?', additional_kwargs={}, example=False),\n AIMessage(content='In June 2023, it is hard to find a rental property in Australia due to several factors. Firstly, vacancy rates have fallen to very low levels across the country since the pandemic, meaning there is a shortage of available rentals. This is particularly evident in cities like Sydney and Melbourne. \\n\\nAdditionally, the departure of investors from the rental market has impacted rental supply. Many investors chose to sell their rental properties during 2020 and 2021, and there are few new investors entering the market to replace them. \\n\\nOn the other hand, demand for rentals has been strong in many parts of the country, especially in inner-city areas. The return of international students, migrants, and office workers to CBDs has led to a surge in demand for rental properties. \\n\\nOverall, these factors have created a tight rental market with low vacancy rates and increasing rental prices, making it difficult for individuals to find a rental property in Australia in June 2023.', additional_kwargs={}, example=False)]\n\n\nSo far, we have the above chat history. Let‚Äôs now ask a follow up question about the home price index and say ‚ÄúHow has the pandemic affected this?‚Äù and we can see the question generator in action.\n\nqa(\"How has the pandemic affected this?\")\n\n{'question': 'How has the pandemic affected this?',\n 'chat_history': 'Human: Why is it so hard to find a rental property in Australia in June 2023?\\nAI: In June 2023, it is hard to find a rental property in Australia due to several factors. Firstly, vacancy rates have fallen to very low levels across the country since the pandemic, meaning there is a shortage of available rentals. This is particularly evident in cities like Sydney and Melbourne. \\n\\nAdditionally, the departure of investors from the rental market has impacted rental supply. Many investors chose to sell their rental properties during 2020 and 2021, and there are few new investors entering the market to replace them. \\n\\nOn the other hand, demand for rentals has been strong in many parts of the country, especially in inner-city areas. The return of international students, migrants, and office workers to CBDs has led to a surge in demand for rental properties. \\n\\nOverall, these factors have created a tight rental market with low vacancy rates and increasing rental prices, making it difficult for individuals to find a rental property in Australia in June 2023.',\n 'answer': 'The given context does not provide specific information about the rental property market in Australia in June 2023.',\n 'generated_question': 'How has the pandemic affected the rental property market in Australia in June 2023?'}\n\n\nAs can be seen above the original question was ‚ÄúHow has the pandemic affected this?‚Äù which got updated to the generated_question - ‚ÄúHow has the pandemic impacted the difficulty in finding a rental property in Australia in June 2023?‚Äù. This was done based on the chat history.\nAnd that‚Äôs all that there is to know about the question generator! We can now move on the document chain which is StuffDocumentsChain.\n\n\n2.4.2 Document chain\nThe stuff documents chain is available as combine_docs_chain attribute from the conversational retrieval chain.\nThe StuffDocumentsChain itself has a LLMChain of it‚Äôs own with the prompt\nsystem_template = \"\"\"Use the following pieces of context to answer the users question. \nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n{context}\"\"\"\nmessages = [\n    SystemMessagePromptTemplate.from_template(system_template),\n    HumanMessagePromptTemplate.from_template(\"{question}\"),\n]\nCHAT_PROMPT = ChatPromptTemplate.from_messages(messages)\n\n\nPROMPT_SELECTOR = ConditionalPromptSelector(\n    default_prompt=PROMPT, conditionals=[(is_chat_model, CHAT_PROMPT)]\n)\nSo, we to our prompt, we pass in the context and a follow up question. It specifically says ‚Äújust say that you don‚Äôt know, don‚Äôt try to make up an answer.‚Äù This is good to limit hallucination.\nWhen we call the StuffDocumentsChain, it does two things - first it calls combine_docs. This method first combines the given input chunks by using separator \\n\\n to generate context.\n    def _get_inputs(self, docs: List[Document], **kwargs: Any) -&gt; dict:\n        # Format each document according to the prompt\n        doc_strings = [format_document(doc, self.document_prompt) for doc in docs]\n        # Join the documents together to put them in the prompt.\n        inputs = {\n            k: v\n            for k, v in kwargs.items()\n            if k in self.llm_chain.prompt.input_variables\n        }\n        inputs[self.document_variable_name] = self.document_separator.join(doc_strings)\n        return inputs\n\n\n    def combine_docs(\n        self, docs: List[Document], callbacks: Callbacks = None, **kwargs: Any\n    ) -&gt; Tuple[str, dict]:\n        inputs = self._get_inputs(docs, **kwargs)\n        # Call predict on the LLM.\n        return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\nGiven our question, remember, we first find the closest chunks to the question. These chunks are then joined together using \\n\\n separator.\n\n\n\n\n\n\nNote\n\n\n\n‚ùì I wonder how things would look like if we numbered the various chunks and passed in the context as bullet points?\n\n\nNext, we just call LLMChain‚Äôs predict method, this generates an answer using a prompt and returns the answer.\nYou know what? That‚Äôs really it! I hope that now you understand completely how context based question answering chatbots work when using langchain. :)"
  },
  {
    "objectID": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html#conclusion",
    "href": "posts/2023-07-27_Document_Question_Answering_with_LangChain.html#conclusion",
    "title": "Demystifying Document Question-Answering Chatbot - A Comprehensive Step-by-Step Tutorial with LangChain",
    "section": "3 Conclusion",
    "text": "3 Conclusion\nIn langchain, once we have a vector database, below lines of code are enough to create a chatbot, that can answer user questions based on some ‚Äúcontext‚Äù.\nimport os\nfrom langchain.vectorstores.chroma import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\n\npersist_directory = \"./db/\"\nopenai_api_key = os.environ['OPENAI_API_KEY']\nembeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\ndb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\nmemory = ConversationBufferMemory(\n    memory_key=\"chat_history\", output_key='answer', return_messages=False)\n\n# create QA chain using `langchain`, database is used as vector store retriever to find \"context\" (using similarity search)\nqa = ConversationalRetrievalChain.from_llm(\n    llm=ChatOpenAI(temperature=0.2, model_name='gpt-3.5-turbo'),\n    chain_type=\"stuff\",\n    retriever=db.as_retriever(),\n    get_chat_history=lambda o:o,\n    memory=memory,\n    return_generated_question=True,\n    verbose=False,\n)\nWe saw all the steps in detail as part of this blog post. We also saw that the ConversationalRetrievalChain consists of two chains:\n\nQuestion generator chain (to generate a new standalone question based on chat history)\nDocuments chain (to combine chunks as context and answer question based on context)\n\nWe saw that both chains consist of llm_chain with different prompts. We even saw the two prompts in detail.\nAnd thus, we uncovered all the magic behind a conversational retrieval chain in langchain. I hope you enjoyed reading this blog post.\nPlease feel to reach out to me on twitter for any follow-up questions!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ruben Stefanus",
    "section": "",
    "text": "Ruben Stefanus is Data Scientist at Halodoc and Co-Founder of Jakarta Artificial Intelligence Research.\nFirst and foremost, I love data and technology. The thought of how data and technology could solve real problems and improve our lives really amazed me ‚ú®\nTherefore, I majored in Electrical Engineering ‚ö°Ô∏è with a Computer Engineering üíª minor, which is at the heart of many modern technologies today\nAll of that led me to go deep into the fields of Data Science\nOutside of work, Ruben enjoys reading book, listening to podcasts, workout in gym, teaching and mentoring people."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Ruben Stefanus",
    "section": "Experience",
    "text": "Experience\n\nData Scientist, Halodoc, Nov 2022 ‚Äì Present\nData Scientist, Finantier, Oct 2021 ‚Äì Nov 2022\nData Scientist, Warung Pintar, Jan 2020 ‚Äì Oct 2021\nTechnical Consultant (Data and Analytics), Berca Hardayaperkasa, Apr 2019 ‚Äì Oct 2019"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Ruben Stefanus",
    "section": "Education",
    "text": "Education\n\nBachelor of Technology - BTech, Electrical Engineering, Maranatha Christian University"
  }
]